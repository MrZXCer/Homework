# 编译原理实验报告

By 161250010 陈俊达

# 1. 目标

实现一个**能解析正则表达式和一些扩展语法的通用词法分析器**，和**使用LALR(1)进行语法分析的语法分析器**，并定义一个能够以上提到的分析器所解析的词法定义文件（`.myl`）和语法定义文件（`.myy`）的格式，并能够做到通过在系统指定的可用Token集合上，从用户给定的**词法定义文件**和**语法定义文件**，实现从**输入文件**到**产生式规约序列**的全过程。

# 2. 内容描述

本实验提供了以下内容：

- 一个Intellij IDEA格式的Java项目，包含了目标中提到的**词法分析器**、**语法分析器**以及对应分析器的一些测例。

- 能被分析器所解析的词法定义文件（`.myl`）和语法定义文件（`.myy`）的格式描述

- 以龙书上例题4.31为依据的输入文件、输出文件、myl词法定义文件和myy语法定义文件，并提供了对应的测例

- 一个C语言子集的输入文件、输出文件、myl词法定义文件和myy语法定义文件。同时还提供了与其中myl和myy等价的flex/bison定义文件以及对应的一些工具程序。提供的词法定义和语法定义已经经过flex和bison测试可以完整无错误的解析提供的输入文件。

# 3. 实现方法

词法定义 --> 词法DFA

语法定义 --> 语法LRDFA

按字节读入输入文件 --字节--> 词法DFA --Token--> 语法LRDFA --> 规约产生式

其中，**词法分析器每读取一个到一个Token即暂停对输入字符的读取，转发给语法分析器进行语法分析；当语法分析器需要更多Token的时候，词法分析才继续对输入字符的读取**，并不是首先生成所有的Token，再一次性交给语法LRDFA进行语法分析。


# 4. 假设

项目中假设只会用到以下类型的Token。若需要更多类型的Token，可在`lex.internal.token.TokenType`枚举类型下增加更多的Token类型。

注意，在产生式中出现Token类型列表所包含的字符串，将被认为是对应类型的终结符。

| Token类型 | 字面量 | 备注 |
| -- | -- | -- |
| VOID | void | |
| INT | int | | 
| RETURN | return | |
| WHILE | while | |
| IF | if | | 
| ELSE | else | |
| ELLIPSIS | ... | |
| EQUAL | == | |
| ASSIGN | = | |
| SEMICOLON | ; | |
| STAR | * | |
| OR_OR | || | |
| LEFT_BRACE | { | |
| RIGHT_BRACE | } | |
| LEFT_PARENTHESIS | ) | |
| RIGHT_PARENTHESIS | ) | |
| COMMA | , | |
| PLUS | + | |
| MINUS | - | |
| DIV | / | |
| INC | ++ | |
| LT | < | |
| LE | <= | |
| IDENTIFIER | id | |
| INT_CONST | int_const | |
| STR_CONST | str_const | |
| IGNORED |  | IGNORED类型的token将不会传送给语法分析器 |
| DOLLAR_R | $R | 当词法分析器结束了所有的读取时，语法分析器无法获得下一个Token，则语法分析器会认为下一个Token是$R | 
| UNKNOWN | # | 若词法分析器分析到UNKNOWN类型的Token，将会抛出LexicalParseException | | 
| EOF | $eof | 词法分析器结束了所有的读取时，将会返回$eof类型的Token |


# 5. 重要数据结构的描述

## 5.1 词法分析器

词法分析器设计到的数据结构有DFA, DFANode, NFA, NFANode, Regex, RegexNode和Rule。

### 5.1.1 Regex和RegexNode

Regex顾名思义代表一个正则表达式，一个Regex是由一个RegexNode的列表所组成的。

一个RegexNode代表一个**正则表达式的符号**，其中**正则表达式的符号**即为正则表达式标准定义中的集几种组成元素，包括以下几种类型：

- 普通字符（包括escaped字符）
- 闭包（*）
- 并集（|）
- 连接（·）
- （转换到中缀表达式后消失）覆盖优先级（(, )）

每个RegexNode都存储了这个**RegexNode的类型**和**它的字面量值**，以及**其对应的优先级**。优先级仅用于连接和并集的优先级选择上，其中连接的连接的优先级高于闭包。这个优先级将会在**正则表达式中缀转后缀**的过程中起作用。

RegexNode通过lombok生成了`equals`和`hashCode`方法，以方便比较两个RegexNode的相等性。两个RegexNode相等当且仅当两个Regex具有相同的类型且具有相同的字面量值。

### 5.1.2 NFA和NFANode

NFA顾名思义代表一个对于一个**正则表达式**的NFA。本系统里的NFA是根据Thompson算法做出来的，而通过Thompson算法做出的**对于一个正则表达式的NFA只有一个结束状态**，故本系统中一个NFA是由代表开始状态的NFANode和一个代表结束状态的NFANode组成的。

NFANode代表一个NFA中的节点，或者说一个NFA的状态。一个NFANode是由一个**以自己为出发边的边的集合**和**对应的正则表达式所对应的token类型**所组成的。

前者（以自己为出发边的边的集合）在Java中的表示形式为`Map<Character, List<NFANode>>`，其key代表边上的字符，value代表通过key字符能够达到的状态的集合。

后者（对应的正则表达式所对应的token类型）对应**在词法定义文件中，满足此正则表达式的字符串应该被语法分析及其后续过程认为的Token的类型**。

NFA的边集没有单独保存，而是通过每个NFANode的**以自己为出发边的边的集合**表示。

### 5.1.3 DFA和DFANode

DFA顾名思义代表一个对于一份**词法定义文件**的DFA，由一个标志开始状态的DFANode和这个DFA所有的接受状态的列表组成。本系统里DFA是以下算法得到的：

1. 分析**词法定义文件**中所有的正则表达式，得到NFA
2. 新增一个开始状态，将这个开始状态和所有NFA的开始状态通过epsilon边相连接
3. 使用**子集构造算法**得到DFA。

DFANode代表一个DFA中的节点，或者说一个DFA的状态。一个DFANode是由**它所对应的NFA状态的集合**、**以自己为出发边的边的集合**和**自己所对应的所有可能的token类型的集合**所组成的。

第一项（所对应的NFA状态的集合）即在子集构造算法中，构成这个DFA状态的NFANode的集合。

第二项（以自己的为出发边的边的集合）在Java中的表示形式为`Map<Character, List<NFANode>>`，其key代表边上的字符，value代表通过key字符能够达到的状态的集合。

第三项（自己所对应的所有可能的token类型的集合）即这个DFA状态对应的NFANode所**对应的正则表达式所对应的token类型**的交集。“这个DFA所对应的NFA状态的集合中包括至少一个结束状态的NFANode”是“DFANode所对应的所有可能的token类型的集合非空”的充要条件。通过保存这个集合能够减少词法分析的时间。

DFA的边集没有单独保存，而是通过每个DFANode的**以自己为出发边的边的集合**表示。

DFANode重写了equals方法。两个DFANode相等当且仅当两个DFANode具有相同的**自己所对应的所有可能的token类型的集合**。虽然NFANode并没有重写equals方法，但是在算法过程中**保证了没有新的NFANode产生**，保证了两个NFANode相等当且仅当它们是同一个对象。

### 5.1.4 Rule

一个Rule代表词法定义文件中定义的转换规则，由**一个正则表达式的字符串**和**对应的Token类型**组成。它仅被用于表示用户的词法定义。


## 5.2 语法分析器

语法分析器用到的数据结构有Symbol, Production, ProductionList, LRItem, LRDFA, LRDFANode。

### 5.2.1 Symbol

Symbol（符号）是语法分析过程的基本单位，有两个属性组成：代表**非终结符名称的ntName**和**代表终结符Token类型的tokenType**。一个符号要么是一个**非终结符**（`nt != null && tokenType == null`），要么是一个**终结符**（`nt == null && tokenType != null`）。通过调用`Symbol.terminal(TokenType)`或者`Symbol.nonterminal(String)`可以分别产生一个终结符或者非终结符实例。

Symbol实现了equals和hashCode方法，两个Symbol相等当且仅当（两个Symbol都是非终结符 && 两个Symbol的非终结符名称相同） || (两个Symbol都是终结符 && 两个Symbol的终结符Token类型相同)。实现hashCode方法允许了将其作为HashMap的Key值，简化了后续的编程。这两个方法都是由lombok实现的。

### 5.2.2 Production

Production代表一个产生式，由**产生式左边的符号(left)**和**产生式右边的符号列表(right)**组成。当产生式右边的符号列表为空的时候，代表这是一个epsilon产生式。

Production实现了equals和hashCode方法。两个Production相等当且仅当两个产生式具有相同的**产生式左边的符号**和**产生式右边的符号列表（包括顺序）**。实现hashCode方法允许了将其作为HashMap的Key值，简化了后续的编程。这两个方法都是由lombok实现的。

### 5.2.3 ProductionList

一个ProductionList代表一个产生式列表，一个ProductionList只允许有一个左边是开始符的产生式。所以，一个产生式列表由**产生式的列表**，**初始产生式(startProduction），即左边是开始符的唯一的产生式**和**开始符（startSymbol）**组成。

为了简化编程，一个ProductionList还提供了计算函数`First(Symbol)`以计算一个符号的First函数值，和`canDeriveToEpsilon(Symbol)`以判断一个符号是否能推出epsilon表达式。为了减少计算时间，这两个函数将会在计算出一个Symbol的结果后，将其结果记录到一个Map中（firstMemo和canDeriveToEpsilonMemo），下次再进行相同的符号的时候，函数将直接从对应的map中直接取值。

### 5.2.4 LRItem

一个LRItem代表一个LR项，由**对应产生式(production)**、**点的位置(dotPosition)**和**向前看符号(lookaheadSymbol)**组成。根据向前看符号是否为null，一个LRItem可能是一个LR(0)项（`lookaheadSymbol == null`），也可能是一个LALR(1)项（`lookaheadSymbol != null`）。

LRItem实现了equals和hashCode方法。两个LRItem相等当且仅当两个LRItem具有相同的**对应产生式**，**点的位置**和**向前看符号**。实现hashCode方法允许了将其作为HashMap的Key值，简化了后续的编程。这两个方法都是由lombok实现的。

### 5.2.5 LRDFA和LRDFANode

LRDFANode代表一个LR自动机中的一个状态，由**代表这个状态的内核(kernel)的LR项（LRItem）的集合**、**组成整个状态的LR项的集合**和**以自己为出发边的边的集合**组成。其中，**以自己为出发边的边的集合**在Java中的表示形式为`Map<Character, List<LRDFANode>>`，其key代表边上的字符，value代表通过key字符能够达到的状态的集合。根据组成其的LR项的类型（LR(0)项或者LALR(1)项），这个LRDFANode可能是LR(0)自动机或者LALR(1)自动机的一个状态。

LRDFANode重写了equals和hashCode方法。两个LRDFANode相等当且仅当它们具有相同的内核。实现hashCode方法允许了将其作为HashMap的Key值，简化了后续的编程。

一个LRDFA代表一个LR确定自动机，由**开始状态(startState)**、**结束状态列表(endStates)**和**所有状态列表(allNodes)**组成。根据其中包含的状态的类型（LR(0)或者LALR(1)），这个自动机可能是LR(0)自动机或者LALR(1)自动机。

# 6. 重要算法描述

### 6.2 构建词法分析DFA

### 6.2.1 词法定义文件 到 转换规则(Rule)集合

首先去掉忽略所有空格行，读到非空格行第一行认为是正则表达式，第二行认为是Token，调用TokenType.valueOf将字符串转换为TokenType。循环这个过程直到输入结束。

### 6.2.2 转换规则 到 NFA

此过程分为4个子过程：**正则表达式字符串预处理**，**加入点符号**，**中缀正则表达式转后缀**和**后缀正则表达式转NFA**。

#### 6.2.2.1 正则表达式字符串预处理

预处理过程会将中括号和字符类的符号转换为只包含字符、*、|和()的标准正则表达式，并将**字符串**转换为**RegexNode的列表**。具体转换规则如下：

1. 遇到左中括号，记录下目前已经进入中括号，并将加入左括号类型的RegexNode。
2. 遇到-字符，获得-前面的RegexNode，再获得之后的一个RegexNode，取得两个字符的ascii码之间的所有字符，通过|连接所有的字符，再在前面和后面各加一个圆括号
3. 遇到\\字符，读取后面一个字符，将查找escapedChar表，将对应的escaped后的字符加入列表。
4. 遇到右中括号，记录已经出了中括号，并加入右括号类型的RegexNode
5. 遇到其他字符，将其字面量的CHAR类型的RegexNode加入列表
6. 最后，如果仍然处于中括号，在后面加一个OR（|）的RegexNode

#### 6.2.2.2 在RegexNode列表中加入点符号

遍历预处理后的RegexNode列表，在满足以下条件的两个符号之间加入点符号，表示这是两个正则表达式相连接的而构成的。

- 左侧符号：是上一个正则表达式的结束 <==> 左侧符号是*, )或者一个普通字符
- 右侧符号：是下一个正则表达式的开始 <==> 右侧符号是(或者一个普通字符

#### 6.2.2.3 中缀正则表达式转后缀

将加入点符号的RegexNode列表转换为后缀表达式，其中

- *是一元运算符，具有最高优先级
- 连接（点）的优先级高于或（|）

#### 6.2.2.4 后缀正则表达式转NFA

使用Thompson算法，将一个后缀正则表达式转换为一个NFA。

1. 所有转换规则对应的NFA --> 一个lNFA

2. lNFA --子集构建算法--> 词法DFA

### 6.3 构建LRDFA

1. 语法定义 --读取--> 产生式列表


2. 产生式列表 --状态内和状态间扩展--> LR(0)自动机

3. LR(0)自动机和产生式列表 --自生成和传播--> LALR(1)自动机

### 6.4 分析过程

1. 输入文件 --> Token

2. Token序列 --> 规约产生式序列


# 7. 可用测试用例

## 7.1 初始化编程环境

1. 使用Jetbrains Intellij IDEA打开项目文件夹
2. 等待gradle下载依赖（本项目依赖[lombok](https://projectlombok.org/)和[junit](https://junit.org/junit4/)，使用gradle进行依赖管理）
3. 安装lombok的IDEA插件
4. 在IDEA Settings -> Build, Execution, Deployment -> Compiler -> Annotation Processors，对整个项目在右边勾选Enable Annotation Processing

## 7.2 运行以龙书例题4.31为依据的集成测试

要运行这个测试，请运行`test/java/IntegrationTest::testExample431`。这个测例将读取`main/java/resources/example4.31`下预先提供的输入文件、词法定义文件和语法分析文件，将输入文件进行词法和语法分析，得到规约序列，并与测试中写好的预期序列进行比较，得出测试结果。

## 7.3 一个C语言子集的运行示例

这个C语言子集提供了flex/bison项目以及基于等价myl, myy定义文件的运行方式。

flex/bison项目中包含了这个C语言子集对应的.l和.y定义文件。要运行flex/bison项目，请参考`CSubsetFlexBison`目录下的说明文档。

与flex/bison项目中提到的C语言子集等价的词法定义文件和语法定义文件，以及和flex/bison项目中示例文件test.c完全相同的示例输入文件均位于`main/java/resources/bigtest`下。测试用例`test/java/IntegrationTest::bigTest`将读取输入文件、词法定义文件和语法分析文件，将输入文件进行词法和语法分析，**输出**规约序列。注意此“测试”由于比较复杂（规约序列中包含200余条产生式），故只提供了程序的输出信息(`output`文件)，并没有进行正确性验证。

## 7.4 单元测试

在开发过程中，单元测试起到了测试一个功能点正确性的作用。本项目中`test/java/LexTest`和`test/java/SyntaxTest`包含了编写词法和语法分析器过程中所用到的单元测试，若有兴趣，可自行运行。

# 8. 遇到的问题和解决方案

# 9. 感受和评论